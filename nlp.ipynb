{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:30.592788Z",
     "start_time": "2025-12-03T07:46:28.882056Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tf-keras",
   "id": "71fb2bf591838345",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\drivers\\python_inter\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\drivers\\python_inter\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\drivers\\python_inter\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\drivers\\python_inter\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\drivers\\python_inter\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\drivers\\python_inter\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\drivers\\python_inter\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\drivers\\python_inter\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\drivers\\python_inter\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\drivers\\python_inter\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\drivers\\python_inter\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\drivers\\python_inter\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\drivers\\python_inter\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\drivers\\python_inter\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\drivers\\python_inter\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\drivers\\python_inter\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:40.364890Z",
     "start_time": "2025-12-03T07:46:32.645938Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\drivers\\python_inter\\Lib\\site-packages\\cupy\\_environment.py:215: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:47.448811Z",
     "start_time": "2025-12-03T07:46:47.438919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "N_HEADS = 8\n",
    "FF_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SPECIAL = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используем устройство: {DEVICE}\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "4c4d4183b96f1427",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем устройство: cuda\n",
      "True\n",
      "12.8\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:53.695554Z",
     "start_time": "2025-12-03T07:46:50.408266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Загружаем датасет\n",
    "df = pd.read_csv(\"data/polyOne_aa.csv\")\n",
    "\n",
    "# Проверяем наличие столбца SMILES (в вашем датасете он называется именно \"smiles\")\n",
    "if \"smiles\" not in df.columns:\n",
    "    raise ValueError(\"Столбец 'smiles' не найден в датасете. Проверьте название столбца.\")\n",
    "\n",
    "# Удаляем строки с пустыми SMILES\n",
    "df = df.dropna(subset=[\"smiles\"])\n",
    "\n",
    "# Полный маппинг всех свойств из датасета с их расшифровкой\n",
    "with open(\"data/user_request_dataset/property_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PROPERTY_MAPPING = json.load(f)\n",
    "\n",
    "# Все числовые признаки из датасета (в оригинальных названиях)\n",
    "NUM_FEATURES = list(PROPERTY_MAPPING.keys())\n",
    "print(NUM_FEATURES)"
   ],
   "id": "dd9418fd4b9a7394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPERTY_MAPPING']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:55.626464Z",
     "start_time": "2025-12-03T07:46:55.618455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def categorize_values(series, property_name):\n",
    "    # Проверка на пропущенные значения\n",
    "    nan_count = series.isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"⚠️ ВНИМАНИЕ: В колонке {property_name} найдено {nan_count} пропущенных значений (NaN). Они будут исключены из анализа.\")\n",
    "        series = series.dropna()\n",
    "    \n",
    "    # Проверка на единственное уникальное значение\n",
    "    if series.nunique() == 1:\n",
    "        print(f\"⚠️ ВНИМАНИЕ: В колонке {property_name} все значения одинаковы ({series.iloc[0]}). Все данные будут отнесены к категории 'среднее'.\")\n",
    "        categories = pd.Series(['среднее'] * len(series), index=series.index)\n",
    "        return categories, ['низкое', 'среднее', 'высокое']\n",
    "    \n",
    "    # Используем квантили для балансировки категорий\n",
    "    low_bound = series.quantile(0.33)\n",
    "    high_bound = series.quantile(0.67)\n",
    "    \n",
    "    # Создание категорий\n",
    "    bins = [-np.inf, low_bound, high_bound, np.inf]\n",
    "    labels = ['низкое', 'среднее', 'высокое']\n",
    "    categories = pd.cut(series, bins=bins, labels=labels)\n",
    "    \n",
    "    # Проверка баланса категорий\n",
    "    value_counts = categories.value_counts().sort_index()\n",
    "    total = len(categories)\n",
    "    \n",
    "    # Вывод информации\n",
    "    print(f\"Диапазон значений для '{property_name}': {series.min():.4f} - {series.max():.4f}\")\n",
    "    print(f\"Пороги категорий: низкое <= {low_bound:.4f}, среднее <= {high_bound:.4f}, высокое > {high_bound:.4f}\")\n",
    "    print(f\"Распределение категорий: низкое={value_counts['низкое']} ({value_counts['низкое']/total:.1%}), \"\n",
    "          f\"среднее={value_counts['среднее']} ({value_counts['среднее']/total:.1%}), \"\n",
    "          f\"высокое={value_counts['высокое']} ({value_counts['высокое']/total:.1%})\")\n",
    "    \n",
    "    return categories, labels\n",
    "\n",
    "# Определяем колонки для обработки\n",
    "common_columns = ['Egc','Egb','Eib','CED','Ei','Eea','nc','ne','epse_6.0','epsc','epse_3.0','epse_1.78','epse_15.0','epse_4.0','epse_5.0','epse_2.0','epse_9.0','epse_7.0','TSb','TSy','epsb','YM','permCH4','permCO2','permH2','permO2','permN2','permHe','Eat','rho','LOI','Xc','Xe','Cp','Td','Tg','Tm']"
   ],
   "id": "16d8c67b2007ecb4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:33:36.156975Z",
     "start_time": "2025-12-03T16:33:33.653697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Применяем категоризацию ко всем колонкам\n",
    "for col in common_columns:\n",
    "    if col in df:\n",
    "        print(f\"\\nОбработка колонки '{col}' в df:\")\n",
    "        values = df[col]\n",
    "        categories, labels = categorize_values(values, {col})\n",
    "        df[f'{col}_category'] = categories\n",
    "        \n",
    "        # Проверка распределения категорий\n",
    "        print(\"Распределение категорий:\")\n",
    "        print(df[f'{col}_category'].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(f\"⚠️ Колонка '{col}' отсутствует\")"
   ],
   "id": "31e2a102665005e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обработка колонки 'Egc' в df:\n",
      "Диапазон значений для '{'Egc'}': 0.4376 - 7.4522\n",
      "Пороги категорий: низкое <= 3.1779, среднее <= 3.6253, высокое > 3.6253\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Egc_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Egb' в df:\n",
      "Диапазон значений для '{'Egb'}': 0.7974 - 7.6678\n",
      "Пороги категорий: низкое <= 2.7763, среднее <= 3.2696, высокое > 3.2696\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Egb_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Eib' в df:\n",
      "Диапазон значений для '{'Eib'}': 2.0447 - 5.0319\n",
      "Пороги категорий: низкое <= 3.2163, среднее <= 3.4721, высокое > 3.4721\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Eib_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'CED' в df:\n",
      "Диапазон значений для '{'CED'}': 32.8621 - 277.6321\n",
      "Пороги категорий: низкое <= 123.5147, среднее <= 137.0887, высокое > 137.0887\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "CED_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Ei' в df:\n",
      "Диапазон значений для '{'Ei'}': 3.8263 - 8.4447\n",
      "Пороги категорий: низкое <= 5.6992, среднее <= 5.9338, высокое > 5.9338\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Ei_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Eea' в df:\n",
      "Диапазон значений для '{'Eea'}': 0.6051 - 4.2849\n",
      "Пороги категорий: низкое <= 2.1308, среднее <= 2.4086, высокое > 2.4086\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Eea_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'nc' в df:\n",
      "Диапазон значений для '{'nc'}': 1.5570 - 2.7411\n",
      "Пороги категорий: низкое <= 1.9009, среднее <= 1.9862, высокое > 1.9862\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "nc_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'ne' в df:\n",
      "Диапазон значений для '{'ne'}': 1.3364 - 2.0117\n",
      "Пороги категорий: низкое <= 1.6096, среднее <= 1.6548, высокое > 1.6548\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "ne_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_6.0' в df:\n",
      "Диапазон значений для '{'epse_6.0'}': 2.1316 - 5.8922\n",
      "Пороги категорий: низкое <= 3.2372, среднее <= 3.5489, высокое > 3.5489\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_6.0_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epsc' в df:\n",
      "Диапазон значений для '{'epsc'}': 2.9256 - 8.0948\n",
      "Пороги категорий: низкое <= 4.2370, среднее <= 4.6185, высокое > 4.6185\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epsc_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_3.0' в df:\n",
      "Диапазон значений для '{'epse_3.0'}': 2.1296 - 6.7141\n",
      "Пороги категорий: низкое <= 3.4577, среднее <= 3.8457, высокое > 3.8457\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_3.0_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_1.78' в df:\n",
      "Диапазон значений для '{'epse_1.78'}': 2.1267 - 7.5074\n",
      "Пороги категорий: низкое <= 3.6962, среднее <= 4.1222, высокое > 4.1222\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_1.78_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_15.0' в df:\n",
      "Диапазон значений для '{'epse_15.0'}': 2.1044 - 4.0632\n",
      "Пороги категорий: низкое <= 2.5660, среднее <= 2.7032, высокое > 2.7032\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170001 (34.0%), высокое=164999 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_15.0_category\n",
      "среднее    170001\n",
      "низкое     165000\n",
      "высокое    164999\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_4.0' в df:\n",
      "Диапазон значений для '{'epse_4.0'}': 2.1316 - 6.3441\n",
      "Пороги категорий: низкое <= 3.3670, среднее <= 3.7257, высокое > 3.7257\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_4.0_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_5.0' в df:\n",
      "Диапазон значений для '{'epse_5.0'}': 2.1315 - 6.0940\n",
      "Пороги категорий: низкое <= 3.3040, среднее <= 3.6370, высокое > 3.6370\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170003 (34.0%), высокое=164997 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_5.0_category\n",
      "среднее    170003\n",
      "низкое     165000\n",
      "высокое    164997\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_2.0' в df:\n",
      "Диапазон значений для '{'epse_2.0'}': 2.1271 - 7.3142\n",
      "Пороги категорий: низкое <= 3.6369, среднее <= 4.0563, высокое > 4.0563\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_2.0_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_9.0' в df:\n",
      "Диапазон значений для '{'epse_9.0'}': 2.1254 - 5.2492\n",
      "Пороги категорий: низкое <= 2.9821, среднее <= 3.2484, высокое > 3.2484\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_9.0_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epse_7.0' в df:\n",
      "Диапазон значений для '{'epse_7.0'}': 2.1294 - 5.6779\n",
      "Пороги категорий: низкое <= 3.1604, среднее <= 3.4538, высокое > 3.4538\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=170000 (34.0%), высокое=164999 (33.0%)\n",
      "Распределение категорий:\n",
      "epse_7.0_category\n",
      "среднее    170000\n",
      "низкое     165001\n",
      "высокое    164999\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'TSb' в df:\n",
      "Диапазон значений для '{'TSb'}': 9.2767 - 173.4423\n",
      "Пороги категорий: низкое <= 77.3708, среднее <= 92.2578, высокое > 92.2578\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "TSb_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'TSy' в df:\n",
      "Диапазон значений для '{'TSy'}': 6.1589 - 115.9169\n",
      "Пороги категорий: низкое <= 63.4367, среднее <= 72.7465, высокое > 72.7465\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "TSy_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'epsb' в df:\n",
      "Диапазон значений для '{'epsb'}': 0.8809 - 458.0467\n",
      "Пороги категорий: низкое <= 8.9514, среднее <= 14.9837, высокое > 14.9837\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "epsb_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'YM' в df:\n",
      "Диапазон значений для '{'YM'}': 183.6800 - 3774.3550\n",
      "Пороги категорий: низкое <= 1901.0263, среднее <= 2168.0004, высокое > 2168.0004\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "YM_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permCH4' в df:\n",
      "Диапазон значений для '{'permCH4'}': -0.4142 - 505.5213\n",
      "Пороги категорий: низкое <= 0.4041, среднее <= 1.2174, высокое > 1.2174\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "permCH4_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permCO2' в df:\n",
      "Диапазон значений для '{'permCO2'}': -0.2747 - 8001.1480\n",
      "Пороги категорий: низкое <= 6.3172, среднее <= 17.5380, высокое > 17.5380\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "permCO2_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permH2' в df:\n",
      "Диапазон значений для '{'permH2'}': 0.0099 - 5001.6313\n",
      "Пороги категорий: низкое <= 16.1104, среднее <= 35.5529, высокое > 35.5529\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "permH2_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permO2' в df:\n",
      "Диапазон значений для '{'permO2'}': -0.2592 - 1123.1646\n",
      "Пороги категорий: низкое <= 1.6790, среднее <= 4.5799, высокое > 4.5799\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "permO2_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permN2' в df:\n",
      "Диапазон значений для '{'permN2'}': -0.3174 - 283.5531\n",
      "Пороги категорий: низкое <= 0.4046, среднее <= 1.1628, высокое > 1.1628\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170001 (34.0%), высокое=164999 (33.0%)\n",
      "Распределение категорий:\n",
      "permN2_category\n",
      "среднее    170001\n",
      "низкое     165000\n",
      "высокое    164999\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'permHe' в df:\n",
      "Диапазон значений для '{'permHe'}': 0.1735 - 2271.0340\n",
      "Пороги категорий: низкое <= 15.3087, среднее <= 31.7356, высокое > 31.7356\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "permHe_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Eat' в df:\n",
      "Диапазон значений для '{'Eat'}': -6.6068 - -4.9589\n",
      "Пороги категорий: низкое <= -6.1014, среднее <= -5.9959, высокое > -5.9959\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Eat_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'rho' в df:\n",
      "Диапазон значений для '{'rho'}': 0.8928 - 1.8701\n",
      "Пороги категорий: низкое <= 1.2545, среднее <= 1.3256, высокое > 1.3256\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "rho_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'LOI' в df:\n",
      "Диапазон значений для '{'LOI'}': 15.5604 - 70.9403\n",
      "Пороги категорий: низкое <= 29.4947, среднее <= 33.8328, высокое > 33.8328\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "LOI_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Xc' в df:\n",
      "Диапазон значений для '{'Xc'}': 5.0850 - 98.7214\n",
      "Пороги категорий: низкое <= 35.8221, среднее <= 45.0675, высокое > 45.0675\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Xc_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Xe' в df:\n",
      "Диапазон значений для '{'Xe'}': 6.7443 - 89.9797\n",
      "Пороги категорий: низкое <= 34.0429, среднее <= 40.6078, высокое > 40.6078\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Xe_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Cp' в df:\n",
      "Диапазон значений для '{'Cp'}': 0.8982 - 2.2665\n",
      "Пороги категорий: низкое <= 1.2247, среднее <= 1.2987, высокое > 1.2987\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Cp_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Td' в df:\n",
      "Диапазон значений для '{'Td'}': 447.3834 - 1088.9125\n",
      "Пороги категорий: низкое <= 686.8036, среднее <= 744.1679, высокое > 744.1679\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Td_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Tg' в df:\n",
      "Диапазон значений для '{'Tg'}': 179.7840 - 724.0656\n",
      "Пороги категорий: низкое <= 480.1603, среднее <= 532.3245, высокое > 532.3245\n",
      "Распределение категорий: низкое=165001 (33.0%), среднее=169999 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Tg_category\n",
      "среднее    169999\n",
      "низкое     165001\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Обработка колонки 'Tm' в df:\n",
      "Диапазон значений для '{'Tm'}': 288.5147 - 816.6113\n",
      "Пороги категорий: низкое <= 570.5060, среднее <= 623.2015, высокое > 623.2015\n",
      "Распределение категорий: низкое=165000 (33.0%), среднее=170000 (34.0%), высокое=165000 (33.0%)\n",
      "Распределение категорий:\n",
      "Tm_category\n",
      "среднее    170000\n",
      "низкое     165000\n",
      "высокое    165000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:58.659381Z",
     "start_time": "2025-12-03T07:46:58.654711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Глобальные конфигурационные данные (вынесены для удобства)\n",
    "GENDER_SYNONYMS = {\n",
    "    'm': {  # мужской род (коэффициент, радиус)\n",
    "        'низкая': ['низкий', 'маленький', 'небольшой', 'скромный', 'невысокий'],\n",
    "        'средняя': ['средний', 'умеренный', 'нормальный', 'стандартный'],\n",
    "        'высокая': ['высокий', 'большой', 'значительный', 'превосходный', 'выдающийся']\n",
    "    },\n",
    "    'f': {  # женский род (активность, плотность)\n",
    "        'низкая': ['низкая', 'маленькая', 'небольшая', 'скромная', 'невысокая'],\n",
    "        'средняя': ['средняя', 'умеренная', 'нормальная', 'стандартная'],\n",
    "        'высокая': ['высокая', 'большая', 'значительная', 'превосходная', 'выдающаяся']\n",
    "    },\n",
    "    'n': {  # средний род (свойство, состояние)\n",
    "        'низкая': ['низкое', 'маленькое', 'небольшое', 'скромное', 'невысокое'],\n",
    "        'средняя': ['среднее', 'умеренное', 'нормальное', 'стандартное'],\n",
    "        'высокая': ['высокое', 'большое', 'значительное', 'превосходное', 'выдающееся']\n",
    "    }\n",
    "}"
   ],
   "id": "78a869b4b1b6fdd9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:58.911450Z",
     "start_time": "2025-12-03T07:46:58.907965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/user_request_dataset/param_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARAM_CONFIG = json.load(f)"
   ],
   "id": "a1b1d494ca4d5703",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:46:59.702060Z",
     "start_time": "2025-12-03T07:46:59.583372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Шаблоны для начала запроса\n",
    "with open(\"data/user_request_dataset/start_phrases.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "START_PHRASES = data[\"START_PHRASES\"]"
   ],
   "id": "69d150d87bb2eaac",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:47:00.127474Z",
     "start_time": "2025-12-03T07:47:00.123464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Цели использования\n",
    "with open(\"data/user_request_dataset/purposes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "PURPOSES = data[\"PURPOSES\"]"
   ],
   "id": "22ed67dd92f910a2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:47:00.591498Z",
     "start_time": "2025-12-03T07:47:00.587923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Соединители для параметров\n",
    "CONNECTORS = [\n",
    "        \", а также \", \", при этом \", \", но при этом \", \", и при этом \", \n",
    "        \", дополнительно \", \", что важно \", \", что критично \", \", еще \"\n",
    "        \", что необходимо \", \", что требуется \", \", что важно для \", \", и \", \", который имеет \"\n",
    "    ]"
   ],
   "id": "62651d73f5928cc4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:51:58.504849Z",
     "start_time": "2025-12-02T19:51:58.500112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_thresholds_sampled(df, sample_size=100_000):\n",
    "    \"\"\"\n",
    "    Рассчитывает пороги на основе выборки данных для экономии памяти\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "    for param in df.columns:\n",
    "        if param not in PARAM_CONFIG:\n",
    "            continue\n",
    "            \n",
    "        # Берем выборку вместо полного датафрейма\n",
    "        sample = df[param].dropna()\n",
    "        if len(sample) > sample_size:\n",
    "            sample = sample.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "        if len(sample) < 10:  # Минимум 10 значений для расчета порогов\n",
    "            continue\n",
    "            \n",
    "        thresholds[param] = {\n",
    "            'low': sample.quantile(0.33),\n",
    "            'high': sample.quantile(0.67)\n",
    "        }\n",
    "    \n",
    "    return thresholds"
   ],
   "id": "300f0effbf43953b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:51:58.510147Z",
     "start_time": "2025-12-02T19:51:58.506377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_value_category_safe(value, thresholds, param):\n",
    "    \"\"\"Безопасное определение категории значения\"\"\"\n",
    "    if pd.isna(value) or param not in thresholds:\n",
    "        return None\n",
    "        \n",
    "    if value <= thresholds[param]['low']:\n",
    "        return 'низкая'\n",
    "    elif value >= thresholds[param]['high']:\n",
    "        return 'высокая'\n",
    "    else:\n",
    "        return 'средняя'"
   ],
   "id": "c4042b74d62f280d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:51:58.518042Z",
     "start_time": "2025-12-02T19:51:58.511151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_batch_memory_efficient(batch_size, df, param_config, thresholds, min_params=2, max_params=5):\n",
    "    \"\"\"\n",
    "    Генерирует батч примеров с минимальным использованием памяти\n",
    "    \n",
    "    Вместо предзагрузки всего датафрейма, используем выборки\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    valid_params = [p for p in param_config.keys() if p in df.columns]\n",
    "    \n",
    "    if not valid_params:\n",
    "        return []\n",
    "    \n",
    "    # Предзагружаем конфиги для всех параметров один раз\n",
    "    param_templates = {}\n",
    "    for param in valid_params:\n",
    "        config = param_config[param]\n",
    "        gender = config['gender']\n",
    "        param_templates[param] = {\n",
    "            'names': config['names'],\n",
    "            'phrases': config['phrases'],\n",
    "            'codes': config['codes'],\n",
    "            'gender': gender,\n",
    "            'adjectives': GENDER_SYNONYMS[gender]\n",
    "        }\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # Выбираем случайные параметры\n",
    "        num_params = random.randint(min_params, min(max_params, len(valid_params)))\n",
    "        selected_params = random.sample(valid_params, num_params)\n",
    "        \n",
    "        param_phrases = []\n",
    "        annotations = []\n",
    "        \n",
    "        for param in selected_params:\n",
    "            config = param_templates[param]\n",
    "            gender = config['gender']\n",
    "            \n",
    "            # Получаем выборку данных для этого параметра\n",
    "            param_data = df[param].dropna()\n",
    "            if len(param_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Берем случайное значение из данных\n",
    "            idx = random.choice(param_data.index)\n",
    "            value = param_data.loc[idx]\n",
    "            \n",
    "            # Определяем категорию\n",
    "            category = get_value_category_safe(value, thresholds, param)\n",
    "            if not category:\n",
    "                continue\n",
    "                \n",
    "            # Генерируем фразу без кэширования всех возможных комбинаций\n",
    "            adjective = random.choice(config['adjectives'][category])\n",
    "            name = random.choice(config['names'])\n",
    "            phrase_template = random.choice(config['phrases'])\n",
    "            phrase = phrase_template.format(adjective=adjective, name=name)\n",
    "            \n",
    "            param_phrases.append(phrase)\n",
    "            annotations.append({\n",
    "                \"param\": param,\n",
    "                \"label\": category,\n",
    "                \"phrase\": phrase,\n",
    "                \"code\": random.choice(config[\"codes\"])\n",
    "            })\n",
    "        \n",
    "        if len(param_phrases) < min_params:\n",
    "            continue\n",
    "            \n",
    "        # Формируем текст запроса\n",
    "        params_text = param_phrases[0]\n",
    "        for i in range(1, len(param_phrases)):\n",
    "            connector = random.choice(CONNECTORS)\n",
    "            params_text += connector + param_phrases[i]\n",
    "        \n",
    "        start_phrase = random.choice(START_PHRASES)\n",
    "        purpose = random.choice(PURPOSES)\n",
    "        text = start_phrase.format(params=params_text, purpose=purpose)\n",
    "        \n",
    "        params_str = \"|\".join([f\"{ann['code']}:{ann['label']}\" for ann in annotations])\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"annotations\": annotations,\n",
    "            \"params_str\": params_str\n",
    "        })\n",
    "    \n",
    "    return results"
   ],
   "id": "5b1e0c9954128fce",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:52:00.527550Z",
     "start_time": "2025-12-02T19:52:00.521233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_batch(batch_size, preloaded_data, min_params=2, max_params=10):\n",
    "    \"\"\"\n",
    "    Генерирует батч примеров за один вызов (оптимизировано для скорости)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    param_cache = preloaded_data['param_cache']\n",
    "    valid_params = preloaded_data['valid_params']\n",
    "    connectors = preloaded_data['connectors']\n",
    "    start_phrases = preloaded_data['start_phrases']\n",
    "    purposes = preloaded_data['purposes']\n",
    "    \n",
    "    if not valid_params:\n",
    "        return []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # Выбираем случайное количество параметров\n",
    "        num_params = random.randint(min_params, min(max_params, len(valid_params)))\n",
    "        selected_params = random.sample(valid_params, num_params)\n",
    "        \n",
    "        param_phrases = []\n",
    "        annotations = []\n",
    "        \n",
    "        for param in selected_params:\n",
    "            # Быстро выбираем случайную категорию для параметра\n",
    "            categories = list(param_cache[param].keys())\n",
    "            if not categories:\n",
    "                continue\n",
    "                \n",
    "            category = random.choice(categories)\n",
    "            # Быстро выбираем случайную фразу из кэша\n",
    "            phrase_data = random.choice(param_cache[param][category])\n",
    "            \n",
    "            param_phrases.append(phrase_data[\"phrase\"])\n",
    "            annotations.append({\n",
    "                \"param\": param,\n",
    "                \"label\": category,\n",
    "                \"phrase\": phrase_data[\"phrase\"],\n",
    "                \"code\": phrase_data[\"code\"]\n",
    "            })\n",
    "        \n",
    "        if len(param_phrases) < min_params:\n",
    "            continue\n",
    "            \n",
    "        # Формируем текст запроса (векторизованно)\n",
    "        params_text = param_phrases[0]\n",
    "        for i in range(1, len(param_phrases)):\n",
    "            connector = random.choice(connectors)\n",
    "            params_text += connector + param_phrases[i]\n",
    "        \n",
    "        start_phrase = random.choice(start_phrases)\n",
    "        purpose = random.choice(purposes)\n",
    "        text = start_phrase.format(params=params_text, purpose=purpose)\n",
    "        \n",
    "        params_str = \"|\".join([f\"{ann['code']}:{ann['label']}\" for ann in annotations])\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"annotations\": annotations,\n",
    "            \"params_str\": params_str\n",
    "        })\n",
    "    \n",
    "    return results"
   ],
   "id": "bc726297edb51df6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:52:01.969812Z",
     "start_time": "2025-12-02T19:52:01.962385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_ner_dataset_memory_efficient(df, param_config, n_samples=20_000, batch_size=500, n_workers=4):\n",
    "    \"\"\"\n",
    "    Генерирует NER-датасет с оптимальным использованием памяти\n",
    "    \"\"\"\n",
    "    # 1. Рассчитываем пороги на выборке\n",
    "    print(\"Рассчитываем пороговые значения на выборке данных...\")\n",
    "    thresholds = calculate_thresholds_sampled(df, sample_size=50_000)\n",
    "    \n",
    "    # 2. Определяем валидные параметры\n",
    "    valid_params = [p for p in param_config.keys() if p in df.columns]\n",
    "    print(f\"Валидные параметры для генерации: {len(valid_params)}\")\n",
    "    \n",
    "    if not valid_params:\n",
    "        print(\"⚠️ Нет валидных параметров для генерации!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 3. Генерируем данные батчами\n",
    "    print(f\"🚀 Генерация {n_samples} примеров с использованием {n_workers} потоков...\")\n",
    "    \n",
    "    total_batches = n_samples // batch_size + (1 if n_samples % batch_size else 0)\n",
    "    all_results = []\n",
    "    \n",
    "    # Используем ThreadPoolExecutor вместо ProcessPoolExecutor для избежания копирования данных\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Создаем список задач для каждого батча\n",
    "        futures = []\n",
    "        for batch_idx in range(total_batches):\n",
    "            current_batch_size = batch_size if batch_idx < total_batches - 1 else n_samples % batch_size\n",
    "            if current_batch_size <= 0:\n",
    "                continue\n",
    "                \n",
    "            future = executor.submit(\n",
    "                generate_batch_memory_efficient,\n",
    "                current_batch_size,\n",
    "                df,\n",
    "                param_config,\n",
    "                thresholds\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Последовательно получаем результаты с прогресс-баром\n",
    "        for future in tqdm(futures, desc=\"Генерация данных\"):\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                \n",
    "                # Принудительно запускаем сборщик мусора после каждого батча\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Ошибка при генерации батча: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # 4. Обрезаем до точного количества запрошенных примеров\n",
    "    all_results = all_results[:n_samples]\n",
    "    \n",
    "    print(f\"✅ Сгенерировано {len(all_results)} примеров\")\n",
    "    return pd.DataFrame(all_results)"
   ],
   "id": "7aacfc0d68d5bd8a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:52:06.928625Z",
     "start_time": "2025-12-02T19:52:06.923133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cached_generate_dataset_memory_efficient(df, param_config, n_samples=20_000, cache_file=\"ner_dataset_cache.pkl\"):\n",
    "    \"\"\"\n",
    "    Генерирует датасет с кэшированием и оптимальным использованием памяти\n",
    "    \"\"\"\n",
    "    # Проверяем наличие кэша\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            print(f\"📥 Загружаем кэшированный датасет из {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Ошибка при загрузке кэша: {str(e)}. Генерируем новый датасет.\")\n",
    "    \n",
    "    # Генерируем новый датасет с управлением памятью\n",
    "    print(f\"🔄 Генерируем новый датасет (оптимизировано для памяти)...\")\n",
    "    \n",
    "    # Работаем только с нужными столбцами для экономии памяти\n",
    "    needed_columns = [p for p in param_config.keys() if p in df.columns]\n",
    "    if not needed_columns:\n",
    "        needed_columns = df.columns[:10].tolist()  # Берем первые 10 столбцов как запасной вариант\n",
    "        \n",
    "    # Создаем маленький датафрейм только с нужными столбцами\n",
    "    small_df = df[needed_columns].copy()\n",
    "    \n",
    "    # Очищаем память\n",
    "    gc.collect()\n",
    "    \n",
    "    # Генерируем датасет\n",
    "    dataset = generate_ner_dataset_memory_efficient(\n",
    "        small_df, \n",
    "        param_config, \n",
    "        n_samples=n_samples,\n",
    "        batch_size=200,  # Уменьшаем размер батча для экономии памяти\n",
    "        n_workers=min(4, os.cpu_count() or 4)  # Не более 4 потоков\n",
    "    )\n",
    "    \n",
    "    # Сохраняем в кэш только если датасет не пустой\n",
    "    if not dataset.empty:\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(dataset, f)\n",
    "            print(f\"💾 Кэш сохранен в {cache_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Не удалось сохранить кэш: {str(e)}\")\n",
    "    \n",
    "    return dataset"
   ],
   "id": "135e1d3fa87d98b9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:42:50.020363Z",
     "start_time": "2025-12-02T19:42:49.997333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Использование - оптимизированная версия для работы с ограниченной памятью\n",
    "print(\"⚡ Генерируем оптимизированный датасет NER с управлением памятью...\")\n",
    "full_dataset = cached_generate_dataset_memory_efficient(\n",
    "    df, \n",
    "    PARAM_CONFIG, \n",
    "    n_samples=100_000,  # Начинаем с меньшего количества\n",
    "    cache_file=\"polymer_ner_dataset_100k.pkl\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Сгенерировано {len(full_dataset)} примеров\")\n",
    "print(full_dataset.head())"
   ],
   "id": "91d7a03d4d7f372c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Генерируем оптимизированный датасет NER с управлением памятью...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cached_generate_dataset_memory_efficient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Использование - оптимизированная версия для работы с ограниченной памятью\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m⚡ Генерируем оптимизированный датасет NER с управлением памятью...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m full_dataset = \u001B[43mcached_generate_dataset_memory_efficient\u001B[49m(\n\u001B[32m      4\u001B[39m     df, \n\u001B[32m      5\u001B[39m     PARAM_CONFIG, \n\u001B[32m      6\u001B[39m     n_samples=\u001B[32m100_000\u001B[39m,  \u001B[38;5;66;03m# Начинаем с меньшего количества\u001B[39;00m\n\u001B[32m      7\u001B[39m     cache_file=\u001B[33m\"\u001B[39m\u001B[33mpolymer_ner_dataset_100k.pkl\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      8\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✅ Сгенерировано \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(full_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m примеров\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m \u001B[38;5;28mprint\u001B[39m(full_dataset.head())\n",
      "\u001B[31mNameError\u001B[39m: name 'cached_generate_dataset_memory_efficient' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T19:02:52.047685Z",
     "start_time": "2025-12-02T19:02:49.949724Z"
    }
   },
   "cell_type": "code",
   "source": "full_dataset.to_csv('data/user_request_dataset/polymer_dataset_new_100к.csv', index=False, encoding='utf-8')",
   "id": "7e36f63568fbbeb7",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:27:35.869898Z",
     "start_time": "2025-12-03T16:27:22.732835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "def normalize_dataset(input_file, output_file):\n",
    "    print(f\"Загрузка {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    fixed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    new_params_strs = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # 1. Парсим аннотации\n",
    "            # Пробуем ast (для питоновских строк) или json\n",
    "            try:\n",
    "                annotations = ast.literal_eval(row['annotations'])\n",
    "            except:\n",
    "                try:\n",
    "                    annotations = json.loads(row['annotations'])\n",
    "                except:\n",
    "                    # Если совсем битая строка, оставляем как есть или пустую\n",
    "                    new_params_strs.append(\"\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "            \n",
    "            if not isinstance(annotations, list):\n",
    "                new_params_strs.append(\"\")\n",
    "                continue\n",
    "\n",
    "            # 2. Собираем новую строку params_str\n",
    "            # Берем четкий 'param' из аннотации и 'label'\n",
    "            # Формат: param1:label1|param2:label2\n",
    "            parts = []\n",
    "            for ann in annotations:\n",
    "                if isinstance(ann, dict) and 'param' in ann and 'label' in ann:\n",
    "                    clean_param = ann['param'] # Например \"Tm\" вместо \"плавл\"\n",
    "                    clean_label = ann['label'] # \"средняя\"\n",
    "                    parts.append(f\"{clean_param}:{clean_label}\")\n",
    "            \n",
    "            new_str = \"|\".join(parts)\n",
    "            new_params_strs.append(new_str)\n",
    "            fixed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в строке {index}: {e}\")\n",
    "            new_params_strs.append(\"\")\n",
    "            error_count += 1\n",
    "\n",
    "    # 3. Заменяем столбец\n",
    "    df['params_str'] = new_params_strs\n",
    "    \n",
    "    # 4. Сохраняем\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Готово! Обработано строк: {fixed_count}. Ошибок: {error_count}\")\n",
    "    print(f\"Файл сохранен как: {output_file}\")\n",
    "    \n",
    "    # Показываем пример\n",
    "    print(\"\\nПример исправления:\")\n",
    "    print(df[['annotations', 'params_str']].head(1).values)\n",
    "\n",
    "# Запуск исправления\n",
    "normalize_dataset(\n",
    "    'data/user_request_dataset/polymer_dataset_new_100к.csv', \n",
    "    'data/user_request_dataset/polymer_dataset_clean.csv'\n",
    ")"
   ],
   "id": "13ccfd036ef93417",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка data/user_request_dataset/polymer_dataset_new_100к.csv...\n",
      "Готово! Обработано строк: 99800. Ошибок: 0\n",
      "Файл сохранен как: data/user_request_dataset/polymer_dataset_clean.csv\n",
      "\n",
      "Пример исправления:\n",
      "[[\"[{'param': 'Tm', 'label': 'средняя', 'phrase': 'с стандартная температура фазового перехода', 'code': 'плавл'}, {'param': 'epse_9.0', 'label': 'высокая', 'phrase': 'с выдающаяся диэлектрической проницаемостью на 9 ГГц', 'code': 'эпс_9'}]\"\n",
      "  'Tm:средняя|epse_9.0:высокая']]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:29:53.497001Z",
     "start_time": "2025-12-03T16:29:51.789815Z"
    }
   },
   "cell_type": "code",
   "source": "full_dataset = pd.read_csv('data/user_request_dataset/polymer_dataset_clean.csv')",
   "id": "e413f06ba2857e61",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T09:52:25.894712Z",
     "start_time": "2025-12-03T09:52:25.884139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_overlaps(entities):\n",
    "    \"\"\"\n",
    "    Фильтрует пересекающиеся сущности, оставляя самые длинные.\n",
    "    entities: список кортежей (start, end, label)\n",
    "    \"\"\"\n",
    "    # Сортируем сущности по длине (от длинных к коротким)\n",
    "    # Если длины равны, сортируем по начальной позиции\n",
    "    sorted_entities = sorted(entities, key=lambda x: (x[1] - x[0], x[0]), reverse=True)\n",
    "    \n",
    "    filtered = []\n",
    "    for candidate in sorted_entities:\n",
    "        cand_start, cand_end, _ = candidate\n",
    "        \n",
    "        is_overlap = False\n",
    "        for existing in filtered:\n",
    "            ex_start, ex_end, _ = existing\n",
    "            \n",
    "            # Проверка пересечения отрезков:\n",
    "            # max(start1, start2) < min(end1, end2) означает пересечение\n",
    "            if max(cand_start, ex_start) < min(cand_end, ex_end):\n",
    "                is_overlap = True\n",
    "                break\n",
    "        \n",
    "        if not is_overlap:\n",
    "            filtered.append(candidate)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def prepare_spacy_training_data(dataset_file):\n",
    "    \"\"\"Подготовка данных для обучения spaCy с фильтрацией пересечений\"\"\"\n",
    "    # Загружаем датасет\n",
    "    df = pd.read_csv(dataset_file)\n",
    "    \n",
    "    # Создаем обучающие данные в формате spaCy\n",
    "    training_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        raw_entities = [] # Сюда собираем всех кандидатов\n",
    "        \n",
    "        # --- Блок парсинга аннотаций (тот же, что и у вас) ---\n",
    "        if 'annotations_str' in row and pd.notna(row['annotations_str']):\n",
    "            try:\n",
    "                annotations = ast.literal_eval(row['annotations_str'])\n",
    "            except (ValueError, SyntaxError):\n",
    "                try:\n",
    "                    annotations = json.loads(row['annotations_str'])\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    print(f\"Не удалось распарсить аннотации: {row['annotations_str']}\")\n",
    "                    continue\n",
    "        elif 'annotations' in row and pd.notna(row['annotations']):\n",
    "            try:\n",
    "                annotations = ast.literal_eval(row['annotations'])\n",
    "            except (ValueError, SyntaxError):\n",
    "                try:\n",
    "                    annotations = json.loads(row['annotations'])\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    print(f\"Не удалось распарсить аннотации: {row['annotations']}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # print(\"Не найдены аннотации в строке\") # Можно закомментировать, чтобы не спамило\n",
    "            continue\n",
    "        \n",
    "        if not isinstance(annotations, list):\n",
    "            continue\n",
    "        \n",
    "        # Создаем кандидатов в аннотации\n",
    "        for ann in annotations:\n",
    "            if not isinstance(ann, dict):\n",
    "                continue\n",
    "                \n",
    "            if 'phrase' not in ann or 'param' not in ann or 'label' not in ann:\n",
    "                continue\n",
    "            \n",
    "            phrase = str(ann['phrase'])\n",
    "            if not phrase: continue\n",
    "            \n",
    "            # !ВАЖНО: text.find находит только ПЕРВОЕ вхождение. \n",
    "            # Если в тексте фраза встречается дважды, а размечена вторая - это проблема.\n",
    "            # Для простого решения пока оставим find, но имейте в виду этот риск.\n",
    "            start = text.find(phrase)\n",
    "            \n",
    "            if start != -1:\n",
    "                end = start + len(phrase)\n",
    "                label = f\"{ann['param']}_{ann['label'].upper()}\"\n",
    "                raw_entities.append((start, end, label))\n",
    "        \n",
    "        # --- ИСПРАВЛЕНИЕ: Фильтруем пересечения перед добавлением ---\n",
    "        if raw_entities:\n",
    "            clean_entities = filter_overlaps(raw_entities)\n",
    "            training_data.append((text, {\"entities\": clean_entities}))\n",
    "    \n",
    "    # Создаем папку, если её нет\n",
    "    os.makedirs('data/user_request_dataset', exist_ok=True)\n",
    "    \n",
    "    # Сохраняем в JSONL формате\n",
    "    with open('data/user_request_dataset/spacy_training_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for text, entities_dict in training_data:\n",
    "            json.dump({\"text\": text, \"entities\": entities_dict[\"entities\"]}, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Подготовлено {len(training_data)} обучающих примеров\")\n",
    "    return training_data"
   ],
   "id": "93d0f9aec2154728",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:47:35.107295Z",
     "start_time": "2025-12-03T07:47:35.101754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Обновленный класс датасета\n",
    "class PropertyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "id": "45f8f8413cef372",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:47:36.012073Z",
     "start_time": "2025-12-03T07:47:36.004065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OptimizedPolymerNERTrainer:\n",
    "    \"\"\"Оптимизированный класс для обучения NER-модели\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"ru_core_news_sm\"):\n",
    "        \"\"\"Инициализация модели\"\"\"\n",
    "        # Проверяем доступность GPU\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        if self.use_gpu:\n",
    "            print(f\"Используется GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"GPU не доступен, используется CPU\")\n",
    "        \n",
    "        try:\n",
    "            self.nlp = spacy.load(model_name)\n",
    "        except OSError:\n",
    "            print(f\"Модель {model_name} не найдена. Создаем базовую модель.\")\n",
    "            self.nlp = spacy.blank(\"ru\")\n",
    "        \n",
    "        # Добавляем NER компонент, если его нет\n",
    "        if \"ner\" not in self.nlp.pipe_names:\n",
    "            self.ner = self.nlp.add_pipe(\"ner\")\n",
    "        else:\n",
    "            self.ner = self.nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    def add_labels_to_ner(self, training_data):\n",
    "        \"\"\"Добавление меток в NER компонент\"\"\"\n",
    "        for _, annotations in training_data:\n",
    "            for ent in annotations.get(\"entities\", []):\n",
    "                if len(ent) >= 3:  # Убедимся, что есть метка\n",
    "                    self.ner.add_label(ent[2])  # ent[2] - это метка\n",
    "    \n",
    "    def train_model_fast(self, training_data, n_iter=20):\n",
    "        \"\"\"Быстрое обучение модели (5 эпох для тестирования)\"\"\"\n",
    "        print(f\"Начинаем быстрое обучение ({n_iter} эпох)...\")\n",
    "        \n",
    "        # Добавляем метки\n",
    "        self.add_labels_to_ner(training_data)\n",
    "        \n",
    "        # Убираем другие компоненты во время обучения\n",
    "        pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "        unaffected_pipes = [pipe for pipe in self.nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "        \n",
    "        # Быстрое обучение\n",
    "        with self.nlp.disable_pipes(*unaffected_pipes):  # Только обучаем NER\n",
    "            # Получаем оптимизатор\n",
    "            optimizer = self.nlp.begin_training()\n",
    "            \n",
    "            # Увеличиваем размер батча для ускорения\n",
    "            batch_size = 8  # Увеличен с 4 до 8\n",
    "            \n",
    "            for i in range(n_iter):\n",
    "                print(f\"Эпоха {i+1}/{n_iter}\")\n",
    "                losses = {}\n",
    "                random.shuffle(training_data)\n",
    "                \n",
    "                # Используем большие батчи для ускорения\n",
    "                batches = spacy.util.minibatch(\n",
    "                    training_data, \n",
    "                    size=spacy.util.compounding(8.0, 64.0, 1.001)  # Увеличен минимальный размер\n",
    "                )\n",
    "                \n",
    "                for batch in batches:\n",
    "                    examples = []\n",
    "                    for text, annotations in batch:\n",
    "                        doc = self.nlp.make_doc(text)\n",
    "                        example = Example.from_dict(doc, annotations)\n",
    "                        examples.append(example)\n",
    "                    \n",
    "                    # Уменьшаем drop для более стабильного обучения\n",
    "                    self.nlp.update(examples, drop=0.3, losses=losses, sgd=optimizer)\n",
    "                \n",
    "                print(f\"Потери: {losses}\")\n",
    "                \n",
    "                # Ранний выход, если потери очень маленькие\n",
    "                if losses.get('ner', 0) < 0.01:\n",
    "                    print(\"Достигнуты минимальные потери, останавливаем обучение\")\n",
    "                    break\n",
    "        \n",
    "        return self.nlp\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"Сохранение обученной модели\"\"\"\n",
    "        self.nlp.to_disk(model_path)\n",
    "        print(f\"Модель сохранена в {model_path}\")"
   ],
   "id": "e728a34bc80e55bd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T07:47:37.904389Z",
     "start_time": "2025-12-03T07:47:37.897371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())\n",
    "print(\"torch.cuda.current_device():\", torch.cuda.current_device())\n",
    "print(\"torch.cuda.get_device_name(0):\", torch.cuda.get_device_name(0))\n",
    "print(\"torch.cuda.memory_allocated():\", torch.cuda.memory_allocated())\n"
   ],
   "id": "5f1a54ec36514b05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n",
      "torch.cuda.current_device(): 0\n",
      "torch.cuda.get_device_name(0): NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "torch.cuda.memory_allocated(): 0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:28:16.301416Z",
     "start_time": "2025-12-03T09:52:35.835849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_with_progress_monitoring():\n",
    "    \"\"\"Обучение с мониторингом прогресса\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Подготовка данных\n",
    "        training_data = prepare_spacy_training_data(full_dataset)\n",
    "        \n",
    "        if not training_data:\n",
    "            print(\"Нет обучающих данных!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Начинаем обучение на {len(training_data)} примерах...\")\n",
    "        print(\"Это займет несколько минут...\")\n",
    "        \n",
    "        # Создание и обучение модели\n",
    "        trainer = OptimizedPolymerNERTrainer()\n",
    "        model = trainer.train_model_fast(training_data, n_iter=20)\n",
    "        \n",
    "        # Создаем директорию для модели\n",
    "        import os\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        \n",
    "        # Сохранение модели\n",
    "        trainer.save_model(\"models/nlp_model_fast\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Обучение завершено за {training_time:.2f} секунд!\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обучении: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Быстрый запуск\n",
    "model = train_with_progress_monitoring()"
   ],
   "id": "fcd13dc6d72a7dde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовлено 99800 обучающих примеров\n",
      "Начинаем обучение на 99800 примерах...\n",
      "Это займет несколько минут...\n",
      "Используется GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Модель ru_core_news_sm не найдена. Создаем базовую модель.\n",
      "Начинаем быстрое обучение (20 эпох)...\n",
      "Эпоха 1/20\n",
      "Потери: {'ner': np.float32(892923.06)}\n",
      "Эпоха 2/20\n",
      "Потери: {'ner': np.float32(618089.4)}\n",
      "Эпоха 3/20\n",
      "Потери: {'ner': np.float32(604502.06)}\n",
      "Эпоха 4/20\n",
      "Потери: {'ner': np.float32(410091.25)}\n",
      "Эпоха 5/20\n",
      "Потери: {'ner': np.float32(218867.47)}\n",
      "Эпоха 6/20\n",
      "Потери: {'ner': np.float32(182427.06)}\n",
      "Эпоха 7/20\n",
      "Потери: {'ner': np.float32(172708.69)}\n",
      "Эпоха 8/20\n",
      "Потери: {'ner': np.float32(175065.67)}\n",
      "Эпоха 9/20\n",
      "Потери: {'ner': np.float32(167125.36)}\n",
      "Эпоха 10/20\n",
      "Потери: {'ner': np.float32(164647.94)}\n",
      "Эпоха 11/20\n",
      "Потери: {'ner': np.float32(166184.69)}\n",
      "Эпоха 12/20\n",
      "Потери: {'ner': np.float32(161960.22)}\n",
      "Эпоха 13/20\n",
      "Потери: {'ner': np.float32(164984.02)}\n",
      "Эпоха 14/20\n",
      "Потери: {'ner': np.float32(163183.56)}\n",
      "Эпоха 15/20\n",
      "Потери: {'ner': np.float32(163230.12)}\n",
      "Эпоха 16/20\n",
      "Потери: {'ner': np.float32(162293.27)}\n",
      "Эпоха 17/20\n",
      "Потери: {'ner': np.float32(160541.94)}\n",
      "Эпоха 18/20\n",
      "Потери: {'ner': np.float32(161003.94)}\n",
      "Эпоха 19/20\n",
      "Потери: {'ner': np.float32(157775.4)}\n",
      "Эпоха 20/20\n",
      "Потери: {'ner': np.float32(159415.47)}\n",
      "Модель сохранена в models/nlp_model_fast\n",
      "Обучение завершено за 16540.31 секунд!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T17:20:20.825849Z",
     "start_time": "2025-12-03T17:20:20.546866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolymerParameterExtractor:\n",
    "    \"\"\"Класс для извлечения параметров из пользовательских запросов\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"models/nlp_model_fast\"):\n",
    "        \"\"\"Загрузка обученной модели\"\"\"\n",
    "        try:\n",
    "            self.nlp = spacy.load(model_path)\n",
    "        except OSError:\n",
    "            print(\"Модель не найдена. Используется базовая модель.\")\n",
    "            self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "    \n",
    "    def extract_parameters(self, text):\n",
    "        \"\"\"Извлечение параметров из текста\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        parameters = {}\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # Парсим метку формата PARAM_CATEGORY\n",
    "            if '_' in ent.label_:\n",
    "                param, category = ent.label_.split('_', 1)\n",
    "                parameters[param] = {\n",
    "                    'category': category.lower(),\n",
    "                    'value': ent.text,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                }\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def get_parameter_info(self):\n",
    "        \"\"\"Получение информации о параметрах\"\"\"\n",
    "        return {\n",
    "            \"Egc\": \"Энергия группового вклада (кДж/моль)\",\n",
    "            \"Egb\": \"Энергия вклада связи (кДж/моль)\",\n",
    "            \"Eib\": \"Энергия внутренней связи (кДж/моль)\",\n",
    "            \"CED\": \"Плотность когезионной энергии (Дж/см³)\",\n",
    "            \"Ei\": \"Энергия ионизации (эВ)\",\n",
    "            \"Eea\": \"Электронное сродство (эВ)\",\n",
    "            \"Eat\": \"Энергия атомизации (кДж/моль)\",\n",
    "            \"nc\": \"Количество атомов углерода\",\n",
    "            \"ne\": \"Количество электронов\",\n",
    "            \"Xc\": \"Степень кристалличности (%)\",\n",
    "            \"Xe\": \"Плотность сшивки (моль/м³)\",\n",
    "            \"epse_6.0\": \"Диэлектрическая проницаемость при 6.0 ГГц\",\n",
    "            \"epsc\": \"Статическая диэлектрическая проницаемость (0 Гц)\",\n",
    "            \"epse_3.0\": \"Диэлектрическая проницаемость при 3.0 ГГц\",\n",
    "            \"epse_1.78\": \"Диэлектрическая проницаемость при 1.78 ГГц\",\n",
    "            \"epse_15.0\": \"Диэлектрическая проницаемость при 15.0 ГГц\",\n",
    "            \"epse_4.0\": \"Диэлектрическая проницаемость при 4.0 ГГц\",\n",
    "            \"epse_5.0\": \"Диэлектрическая проницаемость при 5.0 ГГц\",\n",
    "            \"epse_2.0\": \"Диэлектрическая проницаемость при 2.0 ГГц\",\n",
    "            \"epse_9.0\": \"Диэлектрическая проницаемость при 9.0 ГГц\",\n",
    "            \"epse_7.0\": \"Диэлектрическая проницаемость при 7.0 ГГц\",\n",
    "            \"epsb\": \"Диэлектрическая прочность пробоя (кВ/мм)\",\n",
    "            \"TSb\": \"Прочность при разрыве (МПа)\",\n",
    "            \"TSy\": \"Прочность при текучести (МПа)\",\n",
    "            \"YM\": \"Модуль Юнга (МПа)\",\n",
    "            \"permCH4\": \"Проницаемость для CH₄ (Бэрр)\",\n",
    "            \"permCO2\": \"Проницаемость для CO₂ (Бэрр)\",\n",
    "            \"permH2\": \"Проницаемость для H₂ (Бэрр)\",\n",
    "            \"permO2\": \"Проницаемость для O₂ (Бэрр)\",\n",
    "            \"permN2\": \"Проницаемость для N₂ (Бэрр)\",\n",
    "            \"permHe\": \"Проницаемость для He (Бэрр)\",\n",
    "            \"Cp\": \"Удельная теплоёмкость (Дж/(г·К))\",\n",
    "            \"Td\": \"Температура термического разложения (К)\",\n",
    "            \"Tg\": \"Температура стеклования (К)\",\n",
    "            \"Tm\": \"Температура плавления (К)\",\n",
    "            \"rho\": \"Плотность (г/см³)\",\n",
    "            \"LOI\": \"Порог кислородного индекса (%)\"\n",
    "        }\n",
    "\n",
    "# Использование\n",
    "extractor = PolymerParameterExtractor()"
   ],
   "id": "b68628f0ec3e79e7",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T17:41:41.827065Z",
     "start_time": "2025-12-03T17:41:40.854907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParameterRangeConverter:\n",
    "    \"\"\"Класс для преобразования категорий в числовые диапазоны\"\"\"\n",
    "    \n",
    "    def __init__(self, training_datasets):\n",
    "        \"\"\"Инициализация с исходными датасетами для определения диапазонов\"\"\"\n",
    "        self.thresholds = self._calculate_thresholds(training_datasets)\n",
    "    \n",
    "    def _calculate_thresholds(self, datasets):\n",
    "        \"\"\"Вычисление порогов для каждого параметра\"\"\"\n",
    "        thresholds = {}\n",
    "        \n",
    "        # Объединяем все датасеты для вычисления глобальных порогов\n",
    "        all_data = pd.concat(datasets, ignore_index=True)\n",
    "        \n",
    "        # Вычисляем пороги для всех параметров\n",
    "        for param in set(common_columns):\n",
    "            if param in all_data.columns:\n",
    "                valid_values = all_data[param].dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    low = valid_values.quantile(0.33)\n",
    "                    high = valid_values.quantile(0.67)\n",
    "                    thresholds[param] = {\n",
    "                        'low': low,\n",
    "                        'high': high,\n",
    "                        'min': valid_values.min(),\n",
    "                        'max': valid_values.max(),\n",
    "                        'mean': valid_values.mean(),\n",
    "                        'std': valid_values.std()\n",
    "                    }\n",
    "        \n",
    "        return thresholds\n",
    "    \n",
    "    def get_range_for_category(self, parameter, category):\n",
    "        \"\"\"Получение числового диапазона для параметра и категории\"\"\"\n",
    "        if parameter not in self.thresholds:\n",
    "            return None\n",
    "        \n",
    "        param_info = self.thresholds[parameter]\n",
    "        \n",
    "        if category == 'низкая':\n",
    "            return {\n",
    "                'min': param_info['min'],\n",
    "                'max': param_info['low'],\n",
    "                'description': f\"Низкие значения {parameter}\"\n",
    "            }\n",
    "        elif category == 'средняя':\n",
    "            return {\n",
    "                'min': param_info['low'],\n",
    "                'max': param_info['high'],\n",
    "                'description': f\"Средние значения {parameter}\"\n",
    "            }\n",
    "        elif category == 'высокая':\n",
    "            return {\n",
    "                'min': param_info['high'],\n",
    "                'max': param_info['max'],\n",
    "                'description': f\"Высокие значения {parameter}\"\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_detailed_recommendation(self, parameter, category):\n",
    "        \"\"\"Получение детальной рекомендации\"\"\"\n",
    "        if parameter not in self.thresholds:\n",
    "            return \"Параметр не найден\"\n",
    "        \n",
    "        param_info = self.thresholds[parameter]\n",
    "        range_info = self.get_range_for_category(parameter, category)\n",
    "        \n",
    "        if not range_info:\n",
    "            return \"Неверная категория\"\n",
    "        \n",
    "        # Форматируем значения в зависимости от порядка величины\n",
    "        if abs(param_info['max'] - param_info['min']) > 100:\n",
    "            min_val = f\"{range_info['min']:.2f}\"\n",
    "            max_val = f\"{range_info['max']:.2f}\"\n",
    "        elif abs(param_info['max'] - param_info['min']) > 1:\n",
    "            min_val = f\"{range_info['min']:.3f}\"\n",
    "            max_val = f\"{range_info['max']:.3f}\"\n",
    "        else:\n",
    "            min_val = f\"{range_info['min']:.4f}\"\n",
    "            max_val = f\"{range_info['max']:.4f}\"\n",
    "        \n",
    "        return {\n",
    "            'parameter': parameter,\n",
    "            'category': category,\n",
    "            'range': f\"[{min_val}, {max_val}]\",\n",
    "            'min_value': float(range_info['min']),\n",
    "            'max_value': float(range_info['max']),\n",
    "            'description': range_info['description']\n",
    "        }\n",
    "\n",
    "# Инициализация конвертера\n",
    "converter = ParameterRangeConverter([df])"
   ],
   "id": "77971f0c4b8f86f2",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T17:42:30.529717Z",
     "start_time": "2025-12-03T17:42:30.520972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolymerRecommendationSystem:\n",
    "    \"\"\"Основной класс для обработки пользовательских запросов\"\"\"\n",
    "    \n",
    "    def __init__(self, ner_model_path=\"models/nlp_model_fast\", training_datasets=None):\n",
    "        \"\"\"Инициализация системы\"\"\"\n",
    "        self.extractor = PolymerParameterExtractor(ner_model_path)\n",
    "        self.converter = ParameterRangeConverter(training_datasets) if training_datasets else None\n",
    "    \n",
    "    def process_request(self, user_request):\n",
    "        \"\"\"Обработка пользовательского запроса\"\"\"\n",
    "        print(f\"Обработка запроса: {user_request}\")\n",
    "        \n",
    "        # 1. Извлекаем параметры\n",
    "        parameters = self.extractor.extract_parameters(user_request)\n",
    "        \n",
    "        if not parameters:\n",
    "            return {\n",
    "                \"error\": \"Не удалось извлечь параметры из запроса\",\n",
    "                \"extracted_text\": user_request\n",
    "            }\n",
    "        \n",
    "        # 2. Преобразуем в числовые диапазоны\n",
    "        recommendations = {}\n",
    "        for param, info in parameters.items():\n",
    "            category = info['category']\n",
    "            if self.converter:\n",
    "                recommendation = self.converter.get_detailed_recommendation(param, category)\n",
    "                recommendations[param] = {\n",
    "                    'category': category,\n",
    "                    'extracted_phrase': info['value'],\n",
    "                    'recommendation': recommendation\n",
    "                }\n",
    "            else:\n",
    "                recommendations[param] = {\n",
    "                    'category': category,\n",
    "                    'extracted_phrase': info['value'],\n",
    "                    'recommendation': f\"Для параметра {param} с категорией {category}\"\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            \"original_request\": user_request,\n",
    "            \"extracted_parameters\": parameters,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    \n",
    "    def format_output(self, result):\n",
    "        \"\"\"Форматирование результата для вывода\"\"\"\n",
    "        if not result.get('success', False):\n",
    "            return f\"Ошибка: {result.get('error', 'Неизвестная ошибка')}\"\n",
    "        \n",
    "        output = []\n",
    "        output.append(\"=== РЕКОМЕНДАЦИИ ПО ПОЛИМЕРУ ===\")\n",
    "        output.append(f\"Запрос: {result['original_request']}\")\n",
    "        output.append(\"\")\n",
    "        output.append(\"Извлеченные параметры:\")\n",
    "        \n",
    "        for param, info in result['recommendations'].items():\n",
    "            output.append(f\"  {param}:\")\n",
    "            output.append(f\"    Категория: {info['category']}\")\n",
    "            output.append(f\"    Извлечено: {info['extracted_phrase']}\")\n",
    "            if isinstance(info['recommendation'], dict):\n",
    "                rec = info['recommendation']\n",
    "                output.append(f\"    Диапазон значений: {rec['range']}\")\n",
    "                output.append(f\"    Описание: {rec['description']}\")\n",
    "            else:\n",
    "                output.append(f\"    Рекомендация: {info['recommendation']}\")\n",
    "            output.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(output)"
   ],
   "id": "f3a8d734a113a070",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:44:47.437390Z",
     "start_time": "2025-12-03T14:44:47.431370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model_artifacts(model, converter, extractor, save_dir=\"models/nlp_model_fast/ner\"):\n",
    "    \"\"\"\n",
    "    Сохранение всех компонентов системы (NLP модель + вспомогательные классы)\n",
    "    \n",
    "    Args:\n",
    "        model: Экземпляр OptimizedPolymerNERTrainer\n",
    "        converter: Объект конвертера (например, для перевода значений)\n",
    "        extractor: Объект экстрактора (логика извлечения)\n",
    "        save_dir: Путь к папке для сохранения\n",
    "    \"\"\"\n",
    "    # Создаем директорию, если её нет\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    print(f\"Сохранение артефактов в: {save_dir}...\")\n",
    "\n",
    "    try:\n",
    "        # 1. Сохраняем модель spaCy\n",
    "        nlp = model.nlp if hasattr(model, 'nlp') else model\n",
    "        nlp_path = os.path.join(save_dir, \"spacy_ner_model\")\n",
    "        nlp.to_disk(nlp_path)\n",
    "        print(f\"✓ NLP модель сохранена в {nlp_path}\")\n",
    "\n",
    "        # 2. Сохраняем Python объекты (Converter и Extractor) через pickle\n",
    "        # Мы сохраняем их в один файл словарем, чтобы проще было грузить\n",
    "        artifacts = {\n",
    "            \"converter\": converter,\n",
    "            \"extractor\": extractor,\n",
    "            # Можно добавить метаданные версии\n",
    "            \"version\": \"1.0\",\n",
    "            \"model_type\": \"polymer_ner\"\n",
    "        }\n",
    "        \n",
    "        artifacts_path = os.path.join(save_dir, \"artifacts.pkl\")\n",
    "        with open(artifacts_path, \"wb\") as f:\n",
    "            pickle.dump(artifacts, f)\n",
    "            \n",
    "        print(f\"✓ Вспомогательные компоненты (converter, extractor) сохранены в {artifacts_path}\")\n",
    "        print(\"Все компоненты системы успешно сохранены.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR при сохранении модели: {e}\")\n",
    "        raise e"
   ],
   "id": "2ac394ae2f5f5f12",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T18:01:15.137043Z",
     "start_time": "2025-12-03T18:01:13.930038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Создаем систему\n",
    "system = PolymerRecommendationSystem(\n",
    "    ner_model_path=\"models/nlp_model_fast\",\n",
    "    training_datasets=[df]\n",
    ")\n",
    "\n",
    "# Обрабатываем запрос\n",
    "request = \"Интересует полимер с с маленьким индексом горючести, что важно для с стандартной статической диэлектрической проницаемости, еще , что необходимо низкая проницаемость для гелия, что важно с выдающимся энергетическим вкладом связей, и низкой плотностью, чтобы выдерживать высокие нагрузки\"\n",
    "result = system.process_request(request)\n",
    "print(system.format_output(result))"
   ],
   "id": "14c739dd3f9ee48f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка запроса: Интересует полимер с с маленьким индексом горючести, что важно для с стандартной статической диэлектрической проницаемости, еще , что необходимо низкая проницаемость для гелия, что важно с выдающимся энергетическим вкладом связей, и низкой плотностью, чтобы выдерживать высокие нагрузки\n",
      "=== РЕКОМЕНДАЦИИ ПО ПОЛИМЕРУ ===\n",
      "Запрос: Интересует полимер с с маленьким индексом горючести, что важно для с стандартной статической диэлектрической проницаемости, еще , что необходимо низкая проницаемость для гелия, что важно с выдающимся энергетическим вкладом связей, и низкой плотностью, чтобы выдерживать высокие нагрузки\n",
      "\n",
      "Извлеченные параметры:\n",
      "  LOI:\n",
      "    Категория: низкая\n",
      "    Извлечено: с маленьким индексом горючести\n",
      "    Диапазон значений: [15.560, 29.495]\n",
      "    Описание: Низкие значения LOI\n",
      "\n",
      "  epsc:\n",
      "    Категория: низкая\n",
      "    Извлечено: с стандартной статической диэлектрической проницаемости\n",
      "    Диапазон значений: [2.926, 4.237]\n",
      "    Описание: Низкие значения epsc\n",
      "\n",
      "  permHe:\n",
      "    Категория: низкая\n",
      "    Извлечено: низкая проницаемость для гелия\n",
      "    Диапазон значений: [0.17, 15.31]\n",
      "    Описание: Низкие значения permHe\n",
      "\n",
      "  Egc:\n",
      "    Категория: высокая\n",
      "    Извлечено: с выдающимся энергетическим вкладом связей\n",
      "    Диапазон значений: [3.625, 7.452]\n",
      "    Описание: Высокие значения Egc\n",
      "\n",
      "  rho:\n",
      "    Категория: низкая\n",
      "    Извлечено: низкой плотностью\n",
      "    Диапазон значений: [0.8928, 1.2545]\n",
      "    Описание: Низкие значения rho\n",
      "\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
